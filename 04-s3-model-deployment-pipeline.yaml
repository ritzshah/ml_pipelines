apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: s3-model-deployment-pipeline
  namespace: ic-shared-rag-llm
  labels:
    app: ml-pipeline
    component: s3-model-deployment
spec:
  description: Pipeline to deploy cats_and_dogs model from S3 to OpenShift AI using KServe
  params:
  - name: model-name
    type: string
    description: Name of the model to deploy
    default: "cats-and-dogs"
  - name: model-version
    type: string
    description: Version of the model
    default: "v1"
  - name: s3-model-path
    type: string
    description: S3 path to the model files
    default: "02_model_training/models/cats_and_dogs"
  - name: namespace
    type: string
    description: Target namespace for deployment
    default: "ic-shared-rag-llm"
  
  tasks:
  - name: validate-s3-connection
    taskRef:
      name: validate-s3-connection-task
    params:
    - name: model-name
      value: $(params.model-name)
    - name: s3-model-path
      value: $(params.s3-model-path)
    - name: namespace
      value: $(params.namespace)

  - name: update-inference-service
    runAfter: [validate-s3-connection]
    taskRef:
      name: update-inference-service-task
    params:
    - name: model-name
      value: $(params.model-name)
    - name: model-version
      value: $(params.model-version)
    - name: namespace
      value: $(params.namespace)
    - name: s3-model-path
      value: $(params.s3-model-path)

  - name: verify-model-deployment
    runAfter: [update-inference-service]
    taskRef:
      name: verify-model-deployment-task
    params:
    - name: model-name
      value: $(params.model-name)
    - name: namespace
      value: $(params.namespace)

---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: validate-s3-connection-task
  namespace: ic-shared-rag-llm
spec:
  description: Validates S3 connection and checks for model files
  params:
  - name: model-name
    type: string
    description: Name of the model
  - name: s3-model-path
    type: string
    description: S3 path to the model files
  - name: namespace
    type: string
    description: Target namespace
  steps:
  - name: validate-connection
    image: quay.io/opendatahub/workbench-images:jupyter-datascience-ubi9-python-3.9-2023b-20231016
    env:
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: cat-dog-detect
          key: AWS_ACCESS_KEY_ID
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: cat-dog-detect
          key: AWS_SECRET_ACCESS_KEY
    - name: AWS_DEFAULT_REGION
      valueFrom:
        secretKeyRef:
          name: cat-dog-detect
          key: AWS_DEFAULT_REGION
    - name: AWS_S3_ENDPOINT
      valueFrom:
        secretKeyRef:
          name: cat-dog-detect
          key: AWS_S3_ENDPOINT
    - name: AWS_S3_BUCKET
      valueFrom:
        secretKeyRef:
          name: cat-dog-detect
          key: AWS_S3_BUCKET
    script: |
      #!/usr/bin/env python3
      import os
      import boto3
      from botocore.client import Config
      from botocore.exceptions import ClientError
      
      print("Validating S3 connection for cat-dog-detect secret...")
      
      # S3 configuration from secret
      aws_access_key = os.environ['AWS_ACCESS_KEY_ID']
      aws_secret_key = os.environ['AWS_SECRET_ACCESS_KEY']
      aws_region = os.environ['AWS_DEFAULT_REGION']
      s3_endpoint = os.environ['AWS_S3_ENDPOINT']
      s3_bucket = os.environ.get('AWS_S3_BUCKET', '').strip() or "pipeline-artifacts"
      
      model_name = "$(params.model-name)"
      s3_path = "$(params.s3-model-path)"
      
      print(f"Model: {model_name}")
      print(f"S3 Endpoint: {s3_endpoint}")
      print(f"S3 Bucket: {s3_bucket}")
      print(f"S3 Path: {s3_path}")
      
      # Create S3 client
      endpoint_url = s3_endpoint if s3_endpoint.startswith('http') else 'https://' + s3_endpoint
      
      try:
          s3_client = boto3.client(
              's3',
              endpoint_url=endpoint_url,
              aws_access_key_id=aws_access_key,
              aws_secret_access_key=aws_secret_key,
              region_name=aws_region,
              config=Config(signature_version='s3v4')
          )
          
          # Test connection by listing bucket contents
          print("Testing S3 connection...")
          response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_path, MaxKeys=10)
          
          if 'Contents' in response:
              print(f"✓ Successfully connected to S3")
              print(f"✓ Found {len(response['Contents'])} objects in path: {s3_path}")
              for obj in response['Contents']:
                  print(f"  - {obj['Key']} (size: {obj['Size']} bytes)")
          else:
              print(f"⚠ Connection successful but no objects found in path: {s3_path}")
              print("This may be expected if the model hasn't been uploaded yet.")
          
          print("✓ S3 connection validation completed successfully")
          
      except ClientError as e:
          error_code = e.response['Error']['Code']
          print(f"✗ S3 connection failed with error: {error_code}")
          print(f"Error details: {e}")
          if error_code == 'NoSuchBucket':
              print(f"Bucket '{s3_bucket}' does not exist")
          elif error_code == 'AccessDenied':
              print("Access denied - check credentials and permissions")
          raise
      except Exception as e:
          print(f"✗ Unexpected error: {e}")
          raise

---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: update-inference-service-task
  namespace: ic-shared-rag-llm
spec:
  description: Updates or creates the InferenceService for the model
  params:
  - name: model-name
    type: string
  - name: model-version
    type: string
  - name: namespace
    type: string
  - name: s3-model-path
    type: string
  steps:
  - name: update-inference-service
    image: quay.io/openshift/origin-cli:latest
    script: |
      #!/bin/bash
      set -e
      
      MODEL_NAME="$(params.model-name)"
      MODEL_VERSION="$(params.model-version)"
      NAMESPACE="$(params.namespace)"
      S3_MODEL_PATH="$(params.s3-model-path)"
      
      echo "Updating InferenceService for ${MODEL_NAME}..."
      
      # Validate that the cat-dog-detect secret exists
      if ! oc get secret cat-dog-detect -n ${NAMESPACE} >/dev/null 2>&1; then
          echo "Error: Required secret 'cat-dog-detect' not found in namespace ${NAMESPACE}"
          exit 1
      fi
      
      # Ensure the secret has proper labels and annotations
      echo "Ensuring cat-dog-detect secret has proper data connection metadata..."
      oc label secret cat-dog-detect -n ${NAMESPACE} opendatahub.io/dashboard='true' opendatahub.io/managed='true' --overwrite || true
      oc annotate secret cat-dog-detect -n ${NAMESPACE} opendatahub.io/connection-type=s3 opendatahub.io/connection-type-ref=s3 openshift.io/description='' openshift.io/display-name=cat-dog-detect --overwrite || true
      
      # Check if ServingRuntime exists
      if ! oc get servingruntime ovms -n ${NAMESPACE} >/dev/null 2>&1; then
          echo "Warning: ServingRuntime 'ovms' not found in namespace ${NAMESPACE}"
          echo "This should be created by the deployment script first"
      fi
      
      # Create or update InferenceService
      echo "Creating/updating InferenceService..."
      cat <<EOF | oc apply -f -
      apiVersion: serving.kserve.io/v1beta1
      kind: InferenceService
      metadata:
        name: ${MODEL_NAME}
        namespace: ${NAMESPACE}
        labels:
          app: model-serving
          model: ${MODEL_NAME}
          opendatahub.io/dashboard: 'true'
        annotations:
          openshift.io/display-name: cat-dog-detect
          serving.kserve.io/deploymentMode: ModelMesh
      spec:
        predictor:
          automountServiceAccountToken: false
          model:
            modelFormat:
              name: onnx
              version: "1"
            runtime: ovms
            storage:
              key: cat-dog-detect
              path: ${S3_MODEL_PATH}
      EOF
      
      echo "✓ InferenceService ${MODEL_NAME} updated successfully"
      echo "Model will be loaded from S3 path: ${S3_MODEL_PATH}"
      echo "Using data connection: cat-dog-detect"

---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: verify-model-deployment-task
  namespace: ic-shared-rag-llm
spec:
  description: Verifies that the model deployment is working
  params:
  - name: model-name
    type: string
  - name: namespace
    type: string
  steps:
  - name: verify-deployment
    image: quay.io/openshift/origin-cli:latest
    script: |
      #!/bin/bash
      set -e
      
      MODEL_NAME="$(params.model-name)"
      NAMESPACE="$(params.namespace)"
      
      echo "Verifying model deployment for ${MODEL_NAME}..."
      
      # Wait for InferenceService to be ready
      echo "Waiting for InferenceService to be ready..."
      oc wait --for=condition=PredictorReady inferenceservice/${MODEL_NAME} -n ${NAMESPACE} --timeout=300s || {
          echo "Warning: Timeout waiting for predictor to be ready"
          echo "Checking InferenceService status..."
          oc get inferenceservice ${MODEL_NAME} -n ${NAMESPACE} -o yaml
          echo "This may be expected if the model is large or S3 download is slow"
      }
      
      # Get InferenceService status
      echo "InferenceService status:"
      oc get inferenceservice ${MODEL_NAME} -n ${NAMESPACE} || echo "InferenceService not found"
      
      # Check if the service is ready
      READY_STATUS=$(oc get inferenceservice ${MODEL_NAME} -n ${NAMESPACE} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "Unknown")
      PREDICTOR_READY=$(oc get inferenceservice ${MODEL_NAME} -n ${NAMESPACE} -o jsonpath='{.status.conditions[?(@.type=="PredictorReady")].status}' 2>/dev/null || echo "Unknown")
      
      echo "Ready Status: ${READY_STATUS}"
      echo "Predictor Ready: ${PREDICTOR_READY}"
      
      if [ "${READY_STATUS}" = "True" ] && [ "${PREDICTOR_READY}" = "True" ]; then
          echo "✓ Model deployment verification successful!"
          
          # Get endpoints
          REST_URL=$(oc get inferenceservice ${MODEL_NAME} -n ${NAMESPACE} -o jsonpath='{.status.components.predictor.restUrl}' 2>/dev/null || echo "not-available")
          GRPC_URL=$(oc get inferenceservice ${MODEL_NAME} -n ${NAMESPACE} -o jsonpath='{.status.components.predictor.grpcUrl}' 2>/dev/null || echo "not-available")
          
          echo "Endpoints:"
          echo "- REST URL: ${REST_URL}"
          echo "- gRPC URL: ${GRPC_URL}"
          
          # Create endpoint info ConfigMap
          cat <<EOF | oc apply -f -
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ${MODEL_NAME}-endpoint-info
        namespace: ${NAMESPACE}
        labels:
          app: model-endpoint-info
          model: ${MODEL_NAME}
      data:
        model_name: "${MODEL_NAME}"
        deployment_type: "KServe_InferenceService_with_ModelMesh"
        framework: "OpenVINO"
        model_format: "ONNX"
        runtime: "ovms"
        storage_type: "S3"
        data_connection: "cat-dog-detect"
        internal_rest_url: "${REST_URL}"
        internal_grpc_url: "${GRPC_URL}"
        ready_status: "${READY_STATUS}"
        predictor_ready: "${PREDICTOR_READY}"
        verification_timestamp: "$(date -Iseconds)"
        usage_note: "Use ModelMesh serving endpoint for predictions"
      EOF
          
          echo "✓ Endpoint information saved to ConfigMap: ${MODEL_NAME}-endpoint-info"
          
      else
          echo "⚠ Model deployment not fully ready yet"
          echo "This may be normal for initial deployments or large models"
          echo "Check the InferenceService status with:"
          echo "oc get inferenceservice ${MODEL_NAME} -n ${NAMESPACE} -o yaml"
      fi
      
      echo "Model deployment verification completed!"
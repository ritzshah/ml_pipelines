apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: model-deployment-pipeline
  namespace: ic-shared-rag-llm
  labels:
    app: ml-pipeline
    component: tekton-pipeline
spec:
  description: Pipeline to deploy trained models to OpenVINO runtime
  params:
  - name: model-name
    type: string
    description: Name of the model to deploy
    default: "ml-model"
  - name: model-version
    type: string
    description: Version of the model
    default: "v1"
  - name: model-path
    type: string
    description: Path to the trained model in storage
    default: "/tmp/models/best_model.pkl"
  - name: git-repo-url
    type: string
    description: Git repository URL containing model artifacts
    default: "https://github.com/riteshshah/ml_pipelines.git"
  
  workspaces:
  - name: model-artifacts
    description: Workspace for storing model artifacts
  - name: git-source
    description: Workspace for git repository
  
  tasks:
  - name: fetch-model-artifacts
    taskRef:
      name: git-clone
      kind: ClusterTask
    params:
    - name: url
      value: $(params.git-repo-url)
    - name: revision
      value: main
    workspaces:
    - name: output
      workspace: git-source
  
  - name: download-model
    runAfter: [fetch-model-artifacts]
    taskRef:
      name: download-model-task
    params:
    - name: model-path
      value: $(params.model-path)
    - name: model-name
      value: $(params.model-name)
    workspaces:
    - name: model-storage
      workspace: model-artifacts
    - name: source
      workspace: git-source
  
  - name: convert-to-openvino
    runAfter: [download-model]
    taskRef:
      name: convert-to-openvino-task
    params:
    - name: model-name
      value: $(params.model-name)
    - name: model-version
      value: $(params.model-version)
    workspaces:
    - name: model-storage
      workspace: model-artifacts
  
  - name: create-model-server
    runAfter: [convert-to-openvino]
    taskRef:
      name: create-openvino-server-task
    params:
    - name: model-name
      value: $(params.model-name)
    - name: model-version
      value: $(params.model-version)
    workspaces:
    - name: model-storage
      workspace: model-artifacts
  
  - name: expose-model-endpoint
    runAfter: [create-model-server]
    taskRef:
      name: expose-endpoint-task
    params:
    - name: model-name
      value: $(params.model-name)
    - name: model-version
      value: $(params.model-version)

---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: download-model-task
  namespace: ic-shared-rag-llm
spec:
  description: Downloads the trained model from Elyra pipeline artifacts
  params:
  - name: model-path
    type: string
    description: Path to the model file
  - name: model-name
    type: string
    description: Name of the model
  workspaces:
  - name: model-storage
    description: Workspace to store the downloaded model
  - name: source
    description: Source workspace with git repository
  steps:
  - name: download-model
    image: quay.io/opendatahub/workbench-images:jupyter-datascience-ubi9-python-3.9-2023b-20231016
    script: |
      #!/usr/bin/env python3
      import os
      import shutil
      import joblib
      import json
      from datetime import datetime
      
      print("Starting model download task...")
      
      # Model information
      model_path = "$(params.model-path)"
      model_name = "$(params.model-name)"
      
      # Create model directory structure
      model_dir = f"/workspace/model-storage/{model_name}"
      os.makedirs(model_dir, exist_ok=True)
      
      print(f"Model directory: {model_dir}")
      
      # Check if model exists in the expected location
      source_model_path = f"/workspace/source{model_path}"
      if os.path.exists(source_model_path):
          print(f"Found model at: {source_model_path}")
          # Copy model to model storage
          shutil.copy2(source_model_path, f"{model_dir}/model.pkl")
          print(f"Model copied to: {model_dir}/model.pkl")
      else:
          print(f"Model not found at {source_model_path}, creating placeholder model...")
          # Create a simple placeholder model for demo
          from sklearn.ensemble import RandomForestClassifier
          from sklearn.datasets import make_classification
          
          X, y = make_classification(n_samples=100, n_features=4, random_state=42)
          model = RandomForestClassifier(n_estimators=10, random_state=42)
          model.fit(X, y)
          
          joblib.dump(model, f"{model_dir}/model.pkl")
          print("Placeholder model created and saved")
      
      # Create model metadata
      metadata = {
          "model_name": model_name,
          "model_version": "v1",
          "created_at": datetime.now().isoformat(),
          "model_type": "scikit-learn",
          "framework": "sklearn",
          "input_shape": [1, 4],  # placeholder shape
          "output_shape": [1, 2]   # placeholder shape
      }
      
      with open(f"{model_dir}/metadata.json", "w") as f:
          json.dump(metadata, f, indent=2)
      
      print("Model download task completed successfully!")
      print(f"Model files: {os.listdir(model_dir)}")

---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: convert-to-openvino-task
  namespace: ic-shared-rag-llm
spec:
  description: Converts scikit-learn model to OpenVINO format
  params:
  - name: model-name
    type: string
  - name: model-version
    type: string
  workspaces:
  - name: model-storage
    description: Workspace containing the model
  steps:
  - name: convert-model
    image: openvino/ubuntu20_dev:latest
    script: |
      #!/bin/bash
      set -e
      
      echo "Starting OpenVINO model conversion..."
      
      MODEL_NAME="$(params.model-name)"
      MODEL_VERSION="$(params.model-version)"
      MODEL_DIR="/workspace/model-storage/${MODEL_NAME}"
      OPENVINO_DIR="${MODEL_DIR}/openvino"
      
      echo "Model directory: ${MODEL_DIR}"
      echo "OpenVINO directory: ${OPENVINO_DIR}"
      
      # Create OpenVINO model directory
      mkdir -p "${OPENVINO_DIR}"
      
      # Install required Python packages
      pip install scikit-learn joblib numpy
      
      # Convert sklearn model to ONNX and then to OpenVINO
      cat > /tmp/convert_model.py << 'EOF'
      import joblib
      import numpy as np
      import json
      import os
      from pathlib import Path
      
      def convert_sklearn_to_openvino(model_path, output_dir, model_name):
          """Convert sklearn model to OpenVINO IR format"""
          print(f"Loading model from: {model_path}")
          
          # Load the sklearn model
          model = joblib.load(model_path)
          print(f"Model type: {type(model)}")
          
          # For demo purposes, create a simple OpenVINO-compatible model representation
          # In a real scenario, you would use sklearn-onnx and OpenVINO model optimizer
          
          # Create model configuration
          config = {
              "model_name": model_name,
              "framework": "sklearn",
              "input_shape": [1, 4],  # Adjust based on your model
              "output_shape": [1, 2],
              "precision": "FP32"
          }
          
          # Save model configuration
          config_path = os.path.join(output_dir, f"{model_name}.json")
          with open(config_path, 'w') as f:
              json.dump(config, f, indent=2)
          
          # Copy original model (in real scenario, this would be .xml and .bin files)
          import shutil
          model_copy_path = os.path.join(output_dir, f"{model_name}.pkl")
          shutil.copy2(model_path, model_copy_path)
          
          # Create dummy OpenVINO files for demo
          xml_content = f"""<?xml version="1.0" ?>
      <net name="{model_name}" version="11">
          <layers>
              <layer id="0" name="input" type="Parameter" version="opset1">
                  <data element_type="f32" shape="1,4"/>
                  <output>
                      <port id="0" precision="FP32">
                          <dim>1</dim>
                          <dim>4</dim>
                      </port>
                  </output>
              </layer>
              <layer id="1" name="output" type="Result" version="opset1">
                  <input>
                      <port id="0" precision="FP32">
                          <dim>1</dim>
                          <dim>2</dim>
                      </port>
                  </input>
              </layer>
          </layers>
          <edges>
              <edge from-layer="0" from-port="0" to-layer="1" to-port="0"/>
          </edges>
      </net>"""
          
          xml_path = os.path.join(output_dir, f"{model_name}.xml")
          with open(xml_path, 'w') as f:
              f.write(xml_content)
          
          # Create dummy binary file
          bin_path = os.path.join(output_dir, f"{model_name}.bin")
          with open(bin_path, 'wb') as f:
              f.write(b'dummy_binary_data')
          
          print(f"Model converted successfully!")
          print(f"Files created: {os.listdir(output_dir)}")
          
          return config_path, xml_path, bin_path
      
      if __name__ == "__main__":
          model_path = "/workspace/model-storage/${MODEL_NAME}/model.pkl"
          output_dir = "/workspace/model-storage/${MODEL_NAME}/openvino"
          model_name = "${MODEL_NAME}"
          
          convert_sklearn_to_openvino(model_path, output_dir, model_name)
      EOF
      
      # Run the conversion
      python3 /tmp/convert_model.py
      
      echo "OpenVINO conversion completed!"
      echo "Generated files:"
      ls -la "${OPENVINO_DIR}"

---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: create-openvino-server-task
  namespace: ic-shared-rag-llm
spec:
  description: Creates OpenVINO model server deployment
  params:
  - name: model-name
    type: string
  - name: model-version
    type: string
  workspaces:
  - name: model-storage
    description: Workspace containing the converted model
  steps:
  - name: create-deployment
    image: quay.io/openshift/origin-cli:latest
    script: |
      #!/bin/bash
      set -e
      
      MODEL_NAME="$(params.model-name)"
      MODEL_VERSION="$(params.model-version)"
      NAMESPACE="ic-shared-rag-llm"
      
      echo "Creating OpenVINO model server for ${MODEL_NAME}:${MODEL_VERSION}"
      
      # Create ConfigMap with model files
      cat <<EOF | oc apply -f -
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ${MODEL_NAME}-model-config
        namespace: ${NAMESPACE}
        labels:
          app: openvino-model-server
          model: ${MODEL_NAME}
      data:
        config.json: |
          {
            "model_config_list": [
              {
                "config": {
                  "name": "${MODEL_NAME}",
                  "base_path": "/models/${MODEL_NAME}",
                  "model_version_policy": {
                    "specific": {
                      "versions": ["${MODEL_VERSION}"]
                    }
                  }
                }
              }
            ]
          }
      EOF
      
      # Create Deployment
      cat <<EOF | oc apply -f -
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ${MODEL_NAME}-openvino-server
        namespace: ${NAMESPACE}
        labels:
          app: openvino-model-server
          model: ${MODEL_NAME}
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: openvino-model-server
            model: ${MODEL_NAME}
        template:
          metadata:
            labels:
              app: openvino-model-server
              model: ${MODEL_NAME}
          spec:
            containers:
            - name: openvino-model-server
              image: openvino/model_server:latest
              ports:
              - containerPort: 8080
                name: http
              - containerPort: 8081
                name: grpc
              env:
              - name: LOG_LEVEL
                value: "INFO"
              args:
              - "--config_path=/config/config.json"
              - "--port=8080"
              - "--rest_port=8081"
              volumeMounts:
              - name: model-config
                mountPath: /config
              - name: model-storage
                mountPath: /models
              resources:
                requests:
                  cpu: 500m
                  memory: 1Gi
                limits:
                  cpu: 2
                  memory: 4Gi
              readinessProbe:
                httpGet:
                  path: /v1/config
                  port: 8081
                initialDelaySeconds: 30
                periodSeconds: 10
              livenessProbe:
                httpGet:
                  path: /v1/config
                  port: 8081
                initialDelaySeconds: 60
                periodSeconds: 30
            volumes:
            - name: model-config
              configMap:
                name: ${MODEL_NAME}-model-config
            - name: model-storage
              persistentVolumeClaim:
                claimName: model-artifacts-pvc
      EOF
      
      echo "OpenVINO model server deployment created successfully!"

---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: expose-endpoint-task
  namespace: ic-shared-rag-llm
spec:
  description: Exposes the model server endpoint
  params:
  - name: model-name
    type: string
  - name: model-version
    type: string
  steps:
  - name: create-service-route
    image: quay.io/openshift/origin-cli:latest
    script: |
      #!/bin/bash
      set -e
      
      MODEL_NAME="$(params.model-name)"
      MODEL_VERSION="$(params.model-version)"
      NAMESPACE="ic-shared-rag-llm"
      
      echo "Creating service and route for ${MODEL_NAME}:${MODEL_VERSION}"
      
      # Create Service
      cat <<EOF | oc apply -f -
      apiVersion: v1
      kind: Service
      metadata:
        name: ${MODEL_NAME}-service
        namespace: ${NAMESPACE}
        labels:
          app: openvino-model-server
          model: ${MODEL_NAME}
      spec:
        selector:
          app: openvino-model-server
          model: ${MODEL_NAME}
        ports:
        - name: http
          port: 8080
          targetPort: 8080
          protocol: TCP
        - name: grpc
          port: 8081
          targetPort: 8081
          protocol: TCP
        type: ClusterIP
      EOF
      
      # Create Route
      cat <<EOF | oc apply -f -
      apiVersion: route.openshift.io/v1
      kind: Route
      metadata:
        name: ${MODEL_NAME}-route
        namespace: ${NAMESPACE}
        labels:
          app: openvino-model-server
          model: ${MODEL_NAME}
        annotations:
          openshift.io/host.generated: "true"
      spec:
        to:
          kind: Service
          name: ${MODEL_NAME}-service
          weight: 100
        port:
          targetPort: http
        tls:
          termination: edge
          insecureEdgeTerminationPolicy: Redirect
        wildcardPolicy: None
      EOF
      
      # Wait for route to be created and get the URL
      sleep 10
      
      ROUTE_URL=$(oc get route ${MODEL_NAME}-route -n ${NAMESPACE} -o jsonpath='{.spec.host}')
      
      echo "Model endpoint exposed successfully!"
      echo "Model Server URL: https://${ROUTE_URL}"
      echo "Health Check: https://${ROUTE_URL}/v1/config"
      echo "Model Info: https://${ROUTE_URL}/v1/models/${MODEL_NAME}"
      echo "Prediction: https://${ROUTE_URL}/v1/models/${MODEL_NAME}:predict"
      
      # Create a ConfigMap with endpoint information
      cat <<EOF | oc apply -f -
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: ${MODEL_NAME}-endpoint-info
        namespace: ${NAMESPACE}
        labels:
          app: model-endpoint-info
          model: ${MODEL_NAME}
      data:
        endpoint_url: "https://${ROUTE_URL}"
        model_name: "${MODEL_NAME}"
        model_version: "${MODEL_VERSION}"
        health_check_url: "https://${ROUTE_URL}/v1/config"
        model_info_url: "https://${ROUTE_URL}/v1/models/${MODEL_NAME}"
        prediction_url: "https://${ROUTE_URL}/v1/models/${MODEL_NAME}:predict"
        created_at: "$(date -Iseconds)"
      EOF
      
      echo "Endpoint information stored in ConfigMap: ${MODEL_NAME}-endpoint-info"